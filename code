---
title: "projectv.3"
author: "liang qingyan"
date: "3/20/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
news0 <- read.csv("OnlineNewsPopularity.csv")
names(news0)
dim(news0)
head(news0)

```

## data cleaning

```{r}
#  n_non_stop_words
round(sum(news0$n_non_stop_words==0)/length(news0$n_non_stop_words),2)# 0.97, this variable is almost a single value, we will drop this variable in our model.
```

```{r}
summary(news0$n_non_stop_words)
boxplot(news0$n_non_stop_words,xlab=" n_non_stop_words")
```

```{r}
#install.packages("pastecs")  
library(pastecs)
news0n0 <-news0[,-c(14:19)]
names(news0n0)
news0n1 <-news0n0[,-c(26:33)]
sumad <- stat.desc(news0n1)

options(scipen = 999,digits=1) 
summary_new<-as.data.frame(t(sumad[c(4,5,8,9),c(-1,-2)]))
summary_new
```

```{r}
# kw_min_min:Worst Keyword (Min. Shares). It is weird to have negative value in this variable, we will drop this variable since there are too many wrong values.   
sum(news0$kw_min_min==-1)  #22980 cases
sum(news0$kw_avg_min==-1) # 694
sum(news0$kw_avg_max==-1) # 0
cor(news0[20:22]) # since kw_max_min and kw_avg_min are highly related(r=0.94), we will choose kw_avg_max in our model
variable_name <- c("kw_min_min","kw_avg_min","kw_avg_max")
wrong_value <- c(22980,694,0)

missing_percent <- c(22980/nrow(news0),694/nrow(news0),0)


# n_tokens_content 
boxplot(news0$n_tokens_content,xlab="n_tokens_content")


sum(news0$n_tokens_content==0)/length(news0$n_tokens_content) 
# 0.02979013, 1181 cases
# we will treat 0 as missing value
```

```{r}
# n_unique_tokens is Rate Of Unique Words In The Content, the value range should be (0,1),any value great than 1 should be considered as wrong.
 indt <- which(news0$n_unique_tokens>1)
 wr <- news0[indt,]
wr[,-wr$timedelta] # we can drop this case in our data

library(tidyverse)

sum(news0$n_unique_tokens>1)
sum(news0$n_non_stop_unique_tokens>1)

news<-news0%>%
  dplyr::filter( n_tokens_content>0 & n_unique_tokens<=1) %>%
 dplyr::select(-kw_min_min,-kw_avg_min,-url,-timedelta,-n_non_stop_words ) # drop non-predictive variables, also singlar variable n_non_stop_words
  
Missing_percent <- 1-nrow(news)/nrow(news0) #  0.02981536
Missing_case<- nrow(news0)-nrow(news) # 1182 the total cases we delete
Total_case <-nrow(news0)
Model_case <- nrow(news)

Dt <- data.frame(Missing_percent,Missing_case,Total_case,Model_case)
Dt


```

## data exploration

### data discription

```{r}
names(news)
summary(news)
```

```{r}
indtm <- which(news$shares==843300)
wrm <- news[indtm,]
wrm



indts <- which(news$self_reference_max_shares==843300)
wrs <- news[indts,]
wrs


indtkwm <- which(news$kw_max_max==843300)
wrkwm <- news[indtkwm,]
nrow(wrkwm)/nrow(news)
```

```{r}
library(Hmisc)
par(mfrow = c(3, 2))
hist.data.frame(news) # obviously left or right screwed, a lot of transformations would be needed to both y variable and Xs variables
```

## data relationship exploration

```{r}
# since they are heterogeneous set of features, we will explore the relationship among the predictor variables in each feature.
names(news)
# Word features
pairs(news[1:4],cex=0.2)
cor(news[3:4])
cor(news$n_unique_tokens , news$n_non_stop_unique_tokens) # 0.885152 we will choose n_non_stop_unique_tokens in our model

# Links & References features
#"self_reference_min_shares",self_reference_max_shares" and "self_reference_avg_sharess", "num_hrefs","num_self_hrefs"
pairs(~self_reference_min_shares+self_reference_max_shares+self_reference_avg_sharess+num_hrefs+num_self_hrefs,data=news,cex=0.2) # we can see there is obviously linear relationship among these 3 variables: "self_reference_min_shares", self_reference_max_shares" and "self_reference_avg_sharess"

cor(news[24:26]) #  in this case we will keep "self_reference_avg_sharess" and drop "self_reference_min_shares", self_reference_max_shares" 
cor(news[5:6]) # we will keep these 2 variables


# Keyword features
pairs(news[17:23],cex=0.2) # for variables kw_max_min,kw_min_max,kw_max_max,kw_avg_max,kw_min_avg, kw_max_avg and kw_avg_avg. 
cor(news[17:23]) #we find kw_max_avg and kw_avg_avg are high correlated (0.8164474),we will choose kw_avg_avg, drop kw_max_avg

```

```{r}
# NLP features

ggplot(aes(title_subjectivity,abs_title_subjectivity) ,data=news)+geom_point()+geom_smooth()

ggplot(aes(title_sentiment_polarity,abs_title_sentiment_polarity) ,data=news)+geom_point()+geom_smooth()

sum(news$title_subjectivity < 0.5)/nrow(news) # title_subjectivity is Article text subjectivity score
# abs_title_subjectivity is title_subjectivity absolute difference to 0.5

# in order not to loose information about the direction of this original value, we create a new variable, to indicate if the distance is positive or negative.

news<- mutate(news, title_subjectivity_dis= ifelse( title_subjectivity>=0.5, abs_title_subjectivity, -abs_title_subjectivity))


# in this step we will do same process to abs_title_sentiment_polarity and title_sentiment_polarity
# abs_title_sentiment_polarity is title_sentiment_polarity absolute difference to 0(not 0.5 ), the definition in the document is not correct.
news<- mutate(news, title_sentiment_polarity_dis= ifelse( title_sentiment_polarity>=0, abs_title_sentiment_polarity, -abs_title_sentiment_polarity))

# we will drop abs_title_sentiment_polarity and title_sentiment_polarity, abs_title_subjectivity and title_subjectivity

ggplot(aes(title_subjectivity_dis,log(shares)) ,data=news)+geom_point()+geom_smooth()

ggplot(aes(title_sentiment_polarity_dis,log(shares)) ,data=news)+geom_point()+geom_smooth()


```

```{r}
#"global_subjectivity"          
#[41] "global_sentiment_polarity"     "global_rate_positive_words"   
#[43] "global_rate_negative_words"    "rate_positive_words"          
#[45] "rate_negative_words"           "avg_positive_polarity"        
#[47] "min_positive_polarity"         "max_positive_polarity"        
#[49] "avg_negative_polarity"         "min_negative_polarity"        
#[51] "max_negative_polarity"  

news_npl <- news[,c(56,40:51)]
head(news_npl)
pairs(news_npl[1:7],cex=0.1)
cor(news_npl[2:13])#  rate_negative_words and rate_positive_words are highly correlated (r= -0.9976925), we will drop rate_negative_words.




```

### categorical data exploration

```{r}
lda <- news$LDA_00+news$LDA_01+news$LDA_02+news$LDA_03+news$LDA_04
hist(lda)


## the sum of this five variables =1, In this case we should drop one of these variables,we drop LDA_00 in our model.
```

```{r}
library(ggplot2)
# recoding the dummy variables
news$channel[news$data_channel_is_lifestyle==1] <-"lifestyle"
news$channel[news$data_channel_is_entertainment==1] <-"entertainment"
news$channel[news$data_channel_is_socmed==1] <-"socmed"
news$channel[news$data_channel_is_tech==1] <-"tech"
news$channel[news$data_channel_is_world==1] <-"world"
news$channel[news$data_channel_is_bus==1] <-"business"
news$channel[news$data_channel_is_world==0 & news$data_channel_is_lifestyle==0 & news$data_channel_is_entertainment==0 & news$data_channel_is_socmed==0 & news$data_channel_is_tech==0 & news$data_channel_is_bus==0] <-"other"
news$channel <- factor(news$channel)
ggplot(news,aes(channel))+geom_bar()
ggplot(news, aes(channel,log(shares)))+
  geom_boxplot()

```

```{r}

# recoding the dummy variables
news$weekday[news$weekday_is_monday==1] <-"monday"
news$weekday[news$weekday_is_tuesday==1] <-"tuesday"
news$weekday[news$weekday_is_wednesday==1] <-"wednesday"
news$weekday[news$weekday_is_thursday==1] <-"thursday"
news$weekday[news$weekday_is_friday==1] <-"friday"
news$weekday[news$weekday_is_saturday==1] <-"saturday"
news$weekday[news$weekday_is_sunday==1] <-"sunday"
news$weekday <- factor(news$weekday)

ggplot(news, aes(weekday,log(shares)))+
  geom_boxplot()
ggplot(news, aes(as.factor(is_weekend),log(shares)))+
  geom_boxplot()
# comment: There are no differences through Monday to Friday on log(shares), median log(shares) for weekends is abviously higher than not weekends. We can drop redundant variables and keep is_weekend instead.

```

### drop redundant variables

```{r}
library(tidyverse)
news1 <- news %>%
  dplyr::select(-weekday_is_monday, -weekday_is_tuesday, -weekday_is_wednesday,-weekday_is_thursday, -weekday_is_friday, -weekday_is_saturday,-weekday_is_sunday,-data_channel_is_entertainment,-data_channel_is_bus,-data_channel_is_socmed,-data_channel_is_tech,-data_channel_is_world,-data_channel_is_lifestyle,-data_channel_is_bus,-weekday,-LDA_00,-n_unique_tokens,-kw_max_avg,-self_reference_min_shares,-self_reference_max_shares,-title_subjectivity,-title_sentiment_polarity,-abs_title_subjectivity,-abs_title_sentiment_polarity,-rate_negative_words)

length(news1) 
# we have 36 variables in our final data set 
names(news1)
```

## build the model

### set up training and test data set

```{r}
set.seed(123)
n<-nrow(news1)


index_train <- sample(1:n, round(0.7*n)) # we randomly choose 70% of the data as training data 
newstrain <- news1[index_train,]
newstest <- news1[-index_train,]
```

```{r}
options(scipen=0)
lmnew_full <- lm(shares~.,data=newstrain)
lmnew_full_step <- step(lmnew_full,trace=F)
summary(lmnew_full_step)

par(mfrow=c(1,2))
plot(lmnew_full_step,1:2)
```

## refit the model

### using log- transformation

```{r}
lmnew_full_log <- lm(log(shares)~.,data=newstrain)
summary(lmnew_full_log )

par(mfrow=c(1,2))
plot(lmnew_full_log,1:2)
```

```{r}
table(newstrain$channel)
```

```{r}
dim(newstrain)
```

```{r}

lmnew_full_logs <- step(lmnew_full_log,trace=F)
summary(lmnew_full_logs)
par(mfrow=c(1,2))
plot(lmnew_full_logs,1:2,cex=0.5)
```

### using boxcox

```{r,message=FALSE}
options(scipen=0,digits = 3)
library(MASS)
library(car)
boxcox(lmnew_full, plotit = TRUE, lambda = seq(-0.1, -0.3, by = -0.05))
summary(powerTransform(lmnew_full))

```

```{r}

lmnew_full_t <- lm((((shares ^ -0.22) - 1) / (-0.22))~.,data=newstrain)
summary(lmnew_full_t )

```

```{r}

plot(lmnew_full_t)

```

## improve the model

```{r}
dim(newstrain)
```


```{r}

lmnew_full_ts <- step(lmnew_full_t,trace=F)
summary(lmnew_full_ts)
#par(mfrow=c(1,2))
plot(lmnew_full_ts,1:2)
```

```{r}

plotpoint <- function(z,x,y) {
  ggplot(z,aes(x,y))+
  geom_point(cex=0.2)+
  geom_smooth(se=F)
}

```

```{r}
par(mfrow=c(1,2))
plotpoint(newstrain,sqrt(newstrain$n_tokens_content),((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("sqrt(n_tokens_content)")+ylab("(shares^-0.22) - 1)/(-0.22)")
plotpoint(newstrain,newstrain$n_tokens_content,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("n_tokens_content")+ylab("(shares^-0.22) - 1)/(-0.22)")


```

```{r}


plotpoint(newstrain,newstrain$num_hrefs,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("num_hrefs")+ylab("(shares^-0.22) - 1)/(-0.22)")


```

```{r}

plotpoint(newstrain,newstrain$num_self_hrefs,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("num_self_hrefs")+ylab("(shares^-0.22) - 1)/(-0.22)")



```

```{r}

plotpoint(newstrain,sqrt(newstrain$num_videos),((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("sqrt(num_videos)")+ylab("(shares^-0.22) - 1)/(-0.22)")



```

```{r}


plotpoint(newstrain,sqrt(newstrain$self_reference_avg_sharess),((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("sqrt(self_reference_avg_sharess)")+ylab("(shares^-0.22) - 1)/(-0.22)")



```

```{r}
plotpoint(newstrain,sqrt(newstrain$average_token_length),((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("sqrt(average_token_length)")
```

```{r}
## it seems there is no relationship between n_tokens_title and the transformer response variable, the t-test in the previous model also tell the relationship is not significant, we will drop this variable in our refit model

plotpoint(newstrain,newstrain$n_tokens_title,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("n_tokens_title") 
```

```{r}
# these following variables have linear relationship with our transformed response variable.

plotpoint(newstrain,newstrain$n_non_stop_unique_tokens,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("n_non_stop_unique_tokens")

plotpoint(newstrain,newstrain$LDA_01,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab(" LDA_01")

plotpoint(newstrain,newstrain$ min_positive_polarity,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab(" min_positive_polarity")

plotpoint(newstrain,newstrain$global_subjectivity,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("global_subjectivity")

plotpoint(newstrain,newstrain$avg_negative_polarity,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("avg_negative_polarity")


plotpoint(newstrain,newstrain$title_subjectivity_dis,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab("title_subjectivity_dis")

plotpoint(newstrain,newstrain$ title_sentiment_polarity_dis,((newstrain$shares^-0.22) - 1)/(-0.22)) + xlab(" title_sentiment_polarity_dis")
```

```{r}
# we will refit our model using the findings above,add the polynomial effect and drop the none significant variable n_tokens_title.

lmnew_full_ts1 <- lm(formula = (((shares^-0.22) - 1)/(-0.22)) ~ 
    poly(sqrt(n_tokens_content),2)  + n_non_stop_unique_tokens + 
    sqrt(num_hrefs) + poly(sqrt(num_self_hrefs),2) + poly(sqrt(num_imgs),2) + poly(sqrt(num_videos),3) + average_token_length + 
    num_keywords +channel +sqrt(kw_max_min)  +  + 
    kw_min_max + kw_avg_max  + kw_avg_avg  + poly(sqrt(self_reference_avg_sharess),3) + is_weekend + LDA_01 + LDA_02 + LDA_03 + LDA_04 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    rate_positive_words + min_positive_polarity + avg_negative_polarity + 
    title_subjectivity_dis + title_sentiment_polarity_dis ,
    data = newstrain)

summary(lmnew_full_ts1)
plot(lmnew_full_ts1)

```

```{r}
# in our final model, we can see multiple R-squared:0.1362,	Adjusted R-squared:  0.1351 both of these two values are higher than the model without polynomial effect model multiple R-squared:  0.1195,	Adjusted R-squared:  0.1185
options(scipen=0)
lmnew_final <- step(lmnew_full_ts1,trace=F)
summary(lmnew_final)
plot(lmnew_final,1:4,cex=0.5)


newstrain[c(18830,17019,9695),]

```

```{r}
options(scipen = 999,digits=3) 
#library(faraway)
## check collinearity vif 
## there are no serious collinearity problems in our model
car::vif(lmnew_final)

```

```{r}

### check outliers
p<-35
n<-nrow(newstrain)
plot(hatvalues(lmnew_final), rstandard(lmnew_final),cex=0.5,
xlab='Leverage', ylab='Standardized Residuals')

abline(v=0.08,col="red",lty=2)
abline(h=c(-4,4),col="blue",lty=2)

ind <- which(hatvalues(lmnew_final)>0.08)
newstrain[ind,]


sum(abs(rstandard(lmnew_final))>4) # absolute value that greater than 4
sum(abs(rstandard(lmnew_final))>4)/nrow(newstrain)*100 
```

```{r}
# variable importance 
library(vip)
 vip(lmnew_final, num_features = 35, geom = "point", include_type = TRUE)
 
```

## Evaluate model proformance

### generate predictions for the testing dataset

```{r}

lm_pred_final <- predict(lmnew_final,newstest) # using the final model 
lm_pred_ts <- predict(lmnew_full_ts,newstest) # using the boxcox transformed model with no polynomial effects
lm_pred_log <- predict(lmnew_full_logs,newstest) # using the log transformed model
lm_pred1 <- predict(lmnew_full_step,newstest)

```


```{r}
library(tidyverse)
 library(caret) # for cross-validation methods
# Make predictions and compute the R2, RMSE and MAE
predictions <- lmnew_final %>% predict(newstest)
data.frame( R2 = R2(predictions, ((newstest$shares^-0.22) - 1)/(-0.22)),
            RMSE = RMSE(predictions, ((newstest$shares^-0.22) - 1)/(-0.22)),
            MAE = MAE(predictions, ((newstest$shares^-0.22) - 1)/(-0.22)))
```





### evaluate the model using adjusted Rsquare.

```{r}


# calculate r_square and adjusted r_square for the final model
y<-((newstest$shares^-0.22) - 1)/(-0.22)
SST <-sum((y-mean(y))^2)
Res4<- y-lm_pred_final
SSR4 <-sum(Res4^2)
Rsquared_final <- 1-(SSR4/SST)
Rsquared_final

n <- nrow(newstest)
# p_final <-35
adjust_Rsquared_final <- 1-(SSR4/SST)*((n-1)/(n-35-1))
adjust_Rsquared_final
```

```{r}


 # calculate r_square and adjusted r_square for the boxcox transformed model

y<-((newstest$shares^-0.22) - 1)/(-0.22)
SST <-sum((y-mean(y))^2)
Res3<- y-lm_pred_ts
SSR3 <-sum(Res3^2)
Rsquared_ts <- 1-(SSR3/SST)
Rsquared_ts

adjust_Rsquared_ts <- 1-(SSR3/SST)*((n-1)/(n-31-1))
adjust_Rsquared_ts
```

```{r}


# calculate r_square and adjusted r_square for the log transformed model
y_log <- log(newstest$shares)
SST_log <-sum((y_log-mean(y_log))^2)
Res2<- y_log-lm_pred_log
SSR2 <-sum(Res2^2)
Rsquerd_log <- 1-(SSR2/SST_log)
Rsquerd_log

adjust_Rsquared_log <- 1-(SSR2/SST_log)*((n-1)/(n-30-1))
adjust_Rsquared_log


```


```{r}


 # calculate r_square and adjusted r_square for the first model

y1<-newstest$shares
SST1 <-sum((y1-mean(y1))^2)
Res1<- y1-lm_pred1
SSR1 <-sum(Res1^2)
Rsquared1 <- 1-(SSR1/SST1)
Rsquared1

adjust_Rsquared1 <- 1-(SSR1/SST1)*((n-1)/(n-21-1))
adjust_Rsquared1
```



```{r}
adjust_Rsquared_ts
adjust_Rsquared_log
adjust_Rsquared_final

Rsquared_final
Rsquared_ts
adjust_Rsquared_log

adjust_Rsquared <- c(adjust_Rsquared1,adjust_Rsquared_log,adjust_Rsquared_ts,adjust_Rsquared_final)
Rsquared <-c(Rsquared1,Rsquerd_log,Rsquared_ts,Rsquared_final)
Model <- c("No_Trans Model","Log_Trans Model", "Box_Cox_Trans Model","Final Model")
result <- data.frame(Model,Rsquared,adjust_Rsquared)
result
```







